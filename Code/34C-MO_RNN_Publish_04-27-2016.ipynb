{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yield Oracles:  Predicting Soybean Yields in Brazil Using Satellite and Weather Data\n",
    "\n",
    "This code was written as part of a project to satisfy the [Capstone Course](https://datascience.berkeley.edu/academics/curriculum/synthetic-capstone-course/) requirement of the [UC Berkeley Master of Information and Data Science](https://datascience.berkeley.edu) program.  Our project was completed in April 2016.  For more details and to see our results, please visit our [website](http://amitavadas.github.io/capstone/index.html).  We look forward to your feedback!\n",
    "\n",
    "For questions and comments about this code, please contact the author, Marguerite Oneto, by email at marguerite.oneto@ischool.berkeley.edu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run A Recurrent Convolutional Neural Network (RNN) Using Theano  \n",
    "This code is based on an excellent tutorial by Denny Britz giving an introduction to Recurrent Neural Networks.  Please see the links below for more information.  \n",
    "Reference:  http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/   \n",
    "Reference:  https://github.com/dennybritz/rnn-tutorial-rnnlm   \n",
    "Data for Unit Test:  https://github.com/maoneto/W210/blob/master/Code/data/reddit-comments-2015-trunc.csv   \n",
    "Data for Yield Prediction Test: https://github.com/maoneto/W210/blob/master/Code/data/train_tutiempo_10Rings_max.csv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Theano Utility Functions from utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def shuffle_data(p, X, y):\n",
    "        # shuffle it\n",
    "        shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "        X, y = X[shuffle], y[shuffle]\n",
    "        # divide \n",
    "        n_train = np.round(X.shape[0]*p)\n",
    "        return X[:n_train], y[:n_train], X[n_train:], y[n_train:]\n",
    "\n",
    "def shuffle_data2(X, y, n_train, n_test):\n",
    "    # shuffle it\n",
    "    shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "    X, y = X[shuffle], y[shuffle]\n",
    "    # divide \n",
    "    return X[:n_train], y[:n_train], X[n_train:n_train + n_test], y[n_train:n_train + n_test]\n",
    "    \n",
    "def softmax(x):\n",
    "    xt = np.exp(x - np.max(x))\n",
    "    return xt / np.sum(xt)\n",
    "\n",
    "def save_model_parameters_theano_ut(outfile, model):\n",
    "    U, V, W = model.U.get_value(), model.V.get_value(), model.W.get_value()\n",
    "    np.savez(outfile, U=U, V=V, W=W)\n",
    "#     print \"Saved model parameters to %s.\" %(outfile)\n",
    "   \n",
    "def load_model_parameters_theano_ut(path, model):\n",
    "    npzfile = np.load(path)\n",
    "    U, V, W, t_dim = npzfile[\"U\"], npzfile[\"V\"], npzfile[\"W\"]\n",
    "    model.hidden_dim = U.shape[0]\n",
    "    model.x_dim = U.shape[1]\n",
    "    model.U.set_value(U)\n",
    "    model.V.set_value(V)\n",
    "    model.W.set_value(W)\n",
    "    print \"Loaded model parameters from %s. hidden_dim=%d word_dim=%d\" % (path, U.shape[0], U.shape[1])\n",
    "\n",
    "def save_model_parameters_theano_yp(outfile, model):\n",
    "    U, V, W, t_dim = model.U.get_value(), model.V.get_value(), model.W.get_value(), model.t_dim\n",
    "    np.savez(outfile, U=U, V=V, W=W, t_dim=np.array(t_dim))\n",
    "#     print \"Saved model parameters to %s.\" %(outfile)\n",
    "   \n",
    "def load_model_parameters_theano_yp(path, model):\n",
    "    npzfile = np.load(path)\n",
    "    U, V, W, t_dim = npzfile[\"U\"], npzfile[\"V\"], npzfile[\"W\"], npzfile[\"t_dim\"]\n",
    "    model.hidden_dim = U.shape[0]\n",
    "    model.x_dim = U.shape[1]\n",
    "    model.U.set_value(U)\n",
    "    model.V.set_value(V)\n",
    "    model.W.set_value(W)\n",
    "    model.t_dim = t_dim\n",
    "    print \"Loaded model parameters from %s. hidden_dim=%d x_dim=%d, t_dim=%d\" % (path, U.shape[0], U.shape[1], t_dim)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  RNN Unit Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define RNNTheano Class for Unit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting rnn_theano_ut.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile rnn_theano_ut.py\n",
    "\n",
    "import numpy as np\n",
    "import theano as theano\n",
    "import theano.tensor as T\n",
    "from utils import *\n",
    "import operator\n",
    "\n",
    "class RNNTheanoUT:\n",
    "    \n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # Randomly initialize the network parameters\n",
    "        U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
    "        # Theano: Created shared variables\n",
    "        self.U = theano.shared(name='U', value=U.astype(theano.config.floatX))\n",
    "        self.V = theano.shared(name='V', value=V.astype(theano.config.floatX))\n",
    "        self.W = theano.shared(name='W', value=W.astype(theano.config.floatX))      \n",
    "        # We store the Theano graph here\n",
    "        self.theano = {}\n",
    "        self.__theano_build__()\n",
    "    \n",
    "    def __theano_build__(self):\n",
    "        U, V, W = self.U, self.V, self.W\n",
    "        x = T.ivector('x')\n",
    "        y = T.ivector('y')\n",
    "        def forward_prop_step(x_t, s_t_prev, U, V, W):\n",
    "            s_t = T.tanh(U[:,x_t] + W.dot(s_t_prev))\n",
    "            o_t = T.nnet.softmax(V.dot(s_t))\n",
    "            return [o_t[0], s_t]\n",
    "        [o,s], updates = theano.scan(\n",
    "            forward_prop_step,\n",
    "            sequences=[x],\n",
    "            outputs_info=[None, dict(initial=T.zeros(self.hidden_dim))],\n",
    "            non_sequences=[U, V, W],\n",
    "            truncate_gradient=self.bptt_truncate,\n",
    "            strict=True)\n",
    "        \n",
    "        prediction = T.argmax(o, axis=1)\n",
    "        o_error = T.sum(T.nnet.categorical_crossentropy(o, y))\n",
    "        \n",
    "        # Gradients\n",
    "        dU = T.grad(o_error, U)\n",
    "        dV = T.grad(o_error, V)\n",
    "        dW = T.grad(o_error, W)\n",
    "        \n",
    "        # Assign functions\n",
    "        self.forward_propagation = theano.function([x], o)\n",
    "        self.predict = theano.function([x], prediction)\n",
    "        self.ce_error = theano.function([x, y], o_error)\n",
    "        self.bptt = theano.function([x, y], [dU, dV, dW])\n",
    "        \n",
    "        # SGD\n",
    "        learning_rate = T.scalar('learning_rate')\n",
    "        self.sgd_step = theano.function([x,y,learning_rate], [], \n",
    "                      updates=[(self.U, self.U - learning_rate * dU),\n",
    "                              (self.V, self.V - learning_rate * dV),\n",
    "                              (self.W, self.W - learning_rate * dW)])\n",
    "    \n",
    "    def calculate_total_loss(self, X, Y):\n",
    "        return np.sum([self.ce_error(x,y) for x,y in zip(X,Y)])\n",
    "    \n",
    "    def calculate_loss(self, X, Y):\n",
    "        # Divide calculate_loss by the number of words\n",
    "        num_words = np.sum([len(y) for y in Y])\n",
    "        return self.calculate_total_loss(X,Y)/float(num_words)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Theano RNN for Unit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file...\n",
      "Parsed 101 sentences.\n",
      "Found 689 unique words tokens.\n",
      "Using vocabulary size 400.\n",
      "The least frequent word in our vocabulary is 'way' and appeared 1 times.\n",
      "SGD Step time: 3.376007 milliseconds\n",
      "epoch#: 2, training#: 101, file: save_20160427135528.txt\n",
      "Epoch 1) Loss = 5.9811, ExamplesSeen = 101, trainTime = 0.00 min, endTime = 01:55:28 (saved)\n",
      "Epoch 2) Loss = 5.9665, ExamplesSeen = 202, trainTime = 0.00 min, endTime = 01:55:28 (saved)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/SendIt/anaconda/lib/python2.7/site-packages/theano/scan_module/scan.py:1017: Warning: In the strict mode, all neccessary shared variables must be passed as a part of non_sequences\n",
      "  'must be passed as a part of non_sequences', Warning)\n"
     ]
    }
   ],
   "source": [
    "#! /usr/bin/env python\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from utils import *\n",
    "from rnn_theano_ut import RNNTheanoUT\n",
    "\n",
    "_VOCABULARY_SIZE = int(os.environ.get('VOCABULARY_SIZE', '400'))\n",
    "_HIDDEN_DIM = int(os.environ.get('HIDDEN_DIM', '100'))\n",
    "_LEARNING_RATE = float(os.environ.get('LEARNING_RATE', '0.005'))\n",
    "_NEPOCH = int(os.environ.get('NEPOCH', '2'))\n",
    "_MODEL_FILE = os.environ.get('MODEL_FILE')\n",
    "\n",
    "def train_with_sgd(model, X_train, y_train, X_test, learning_rate=0.005, nepoch=1, evaluate_loss_after=1):\n",
    "    # Create file to save test dataset predictions\n",
    "    filename = 'save_' + datetime.now().strftime(\"%Y%m%d%H%M%S\") + '.txt'\n",
    "    print 'epoch#: %d, training#: %s, file: %s' %(nepoch, y_train.shape[0], filename)        \n",
    "    # We keep track of the losses so we can plot them later\n",
    "    min_loss = 100000\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    for epoch in range(nepoch):\n",
    "        # Adjust the learning rate if loss increases\n",
    "        if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "            learning_rate = learning_rate * 0.5  \n",
    "            print \"Setting learning rate to %f\" % learning_rate\n",
    "        # Train the model\n",
    "        epoch_start = time.time()\n",
    "        for i in range(len(y_train)):\n",
    "            # One SGD step\n",
    "            model.sgd_step(X_train[i], y_train[i], learning_rate)\n",
    "            num_examples_seen += 1\n",
    "        epoch_time = time.time() - epoch_start        \n",
    "        # Calculate loss\n",
    "        loss = model.calculate_loss(X_train, y_train)\n",
    "        losses.append((num_examples_seen, loss))\n",
    "        # If loss is a new minimum, then make predictions on the test dataset and save model parameters\n",
    "        isSaved = ''\n",
    "        if loss < min_loss:\n",
    "            min_loss = loss\n",
    "            X_test_predictions = []\n",
    "            for i in range(len(X_test_predictions)):\n",
    "                X_test_predictions.append(model.predict(X_test[i]))\n",
    "            np.savetxt(filename, X_test_predictions)\n",
    "            isSaved = '(saved)'\n",
    "            # ADDED! Saving model parameters\n",
    "            save_model_parameters_theano_ut(\"./data/rnn-theano-%d-%d.npz\" % (model.hidden_dim, model.word_dim), model)\n",
    "        print 'Epoch %d) Loss = %.4f, ExamplesSeen = %d, trainTime = %.2f min, endTime = %s %s' %(epoch+1, loss, num_examples_seen, epoch_time/60, time.strftime(\"%I:%M:%S\"), isSaved)\n",
    "        \n",
    "    return X_test_predictions, losses\n",
    "        \n",
    "        \n",
    "vocabulary_size = _VOCABULARY_SIZE\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "\n",
    "# Read the data and append SENTENCE_START and SENTENCE_END tokens\n",
    "print \"Reading CSV file...\"\n",
    "with open('data/reddit-comments-2015-trunc.csv', 'rb') as f:\n",
    "    reader = csv.reader(f, skipinitialspace=True)\n",
    "    reader.next()\n",
    "    # Split full comments into sentences\n",
    "    sentences = itertools.chain(*[nltk.sent_tokenize(x[0].decode('utf-8').lower()) for x in reader])\n",
    "    # Append SENTENCE_START and SENTENCE_END\n",
    "    sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "print \"Parsed %d sentences.\" % (len(sentences))\n",
    "    \n",
    "# Tokenize the sentences into words\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Count the word frequencies\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print \"Found %d unique words tokens.\" % len(word_freq.items())\n",
    "\n",
    "# Get the most common words and build index_to_word and word_to_index vectors\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    "\n",
    "print \"Using vocabulary size %d.\" % vocabulary_size\n",
    "print \"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1])\n",
    "\n",
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "\n",
    "# Create the training data\n",
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])\n",
    "X_test = X_train[0:10]\n",
    "\n",
    "model = RNNTheanoUT(vocabulary_size, hidden_dim=_HIDDEN_DIM)\n",
    "t1 = time.time()\n",
    "model.sgd_step(X_train[10], y_train[10], _LEARNING_RATE)\n",
    "t2 = time.time()\n",
    "print \"SGD Step time: %f milliseconds\" % ((t2 - t1) * 1000.)\n",
    "\n",
    "if _MODEL_FILE != None:\n",
    "    load_model_parameters_theano_ut(_MODEL_FILE, model)\n",
    "\n",
    "y_hat, losses = train_with_sgd(model, X_train, y_train, X_test, nepoch=_NEPOCH, learning_rate=_LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN for Yield Prediction    \n",
    "We made several changes to the code above to improve performance and to adapt it to our yield prediction problem.   \n",
    "1. The code above solves a classification problem, whereas we needed to predict the value of a continuous variable.  In order to do this, we made changes to the last calculation in the forward propagation step (Line 57 in rnn_theano_yp.py), to the objective function (Line 100 in rnn_theano_yp.py) and to the prediction calculation (Line 99 in rnn_theano_yp.py).   \n",
    "2. To control for overfitting, we added dropout to the training (Lines 48-56 in rnn_theano_yp.py).   \n",
    "3. To speed up training, we used rmsprop, which allows for an adaptive learning rate for each parameter.  We used the method and code from here:  http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training and Test Datasets for Yield Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train and X_test Shape:\n",
      "(958, 13, 23, 1) (240, 13, 23, 1)\n",
      "Y_train, Y_test, log_Y_train, log_Y_test Shape:\n",
      "(958, 13) (240, 13) (958, 13) (240, 13)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "num_images = 13\n",
    "with open('train_tutiempo_10Rings_max.csv', 'r') as csvfile:\n",
    "    datareader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in datareader:\n",
    "        label = row.pop()  # pop the last element in the list which is the label (yield_calc)\n",
    "        if float(label) != 0.0: # all of this is done to get x into the right dimensions for SGD\n",
    "            line = []\n",
    "            for i in range(num_images):\n",
    "                value = []\n",
    "                for j in range(len(ast.literal_eval(row[i]))):\n",
    "                    value.append([ast.literal_eval(row[i])[j]])\n",
    "                line.append(value)\n",
    "            X.append(line)\n",
    "            Y.append(len(X[0])*[label]) # output/prediction at each t, o_t, is the yield\n",
    "X = np.array(X).astype(np.float)\n",
    "Y = np.array(Y).astype(np.float)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=22)\n",
    "\n",
    "log_Y_train = np.log(Y_train)\n",
    "log_Y_test = np.log(Y_test)\n",
    "\n",
    "print \"X_train and X_test Shape:\"\n",
    "print X_train.shape, X_test.shape\n",
    "\n",
    "print \"Y_train, Y_test, log_Y_train, log_Y_test Shape:\"\n",
    "print Y_train.shape, Y_test.shape, log_Y_train.shape, log_Y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Define RNNTheano Class for Yield Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting rnn_theano_yp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile rnn_theano_yp.py\n",
    "\n",
    "import numpy as np\n",
    "import theano as theano\n",
    "import theano.tensor as T\n",
    "from theano import printing\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "from utils import *\n",
    "import operator\n",
    "\n",
    "theano.exception_verbosity='high'\n",
    "theano.mode='FAST_COMPILE'\n",
    "theano.allow_gc=False\n",
    "theano.optimizer='fast_compile'\n",
    "theano.config.compute_test_value = 'off'\n",
    "\n",
    "class RNNTheanoYP:\n",
    "    \n",
    "    def __init__(self, x_dim=1, t_dim=1, hidden_dim=100, bptt_truncate=4):\n",
    "        # Assign instance variables\n",
    "        self.x_dim = x_dim\n",
    "        self.t_dim = t_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # Randomly initialize the network parameters\n",
    "        U = np.random.uniform(-np.sqrt(1./x_dim), np.sqrt(1./x_dim), (hidden_dim, x_dim))\n",
    "        V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (1, hidden_dim))\n",
    "        W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
    "        # Theano: Create shared variables\n",
    "        self.U = theano.shared(name='U', value=U.astype(theano.config.floatX))\n",
    "        self.V = theano.shared(name='V', value=V.astype(theano.config.floatX))\n",
    "        self.W = theano.shared(name='W', value=W.astype(theano.config.floatX))   \n",
    "        # SGD / rmsprop: Initialize parameters\n",
    "        self.mU = theano.shared(name='mU', value=np.zeros(U.shape).astype(theano.config.floatX))\n",
    "        self.mV = theano.shared(name='mV', value=np.zeros(V.shape).astype(theano.config.floatX))\n",
    "        self.mW = theano.shared(name='mW', value=np.zeros(W.shape).astype(theano.config.floatX))\n",
    "        # We store the Theano graph here\n",
    "        self.theano = {}\n",
    "        self.__theano_build__()\n",
    "\n",
    "    \n",
    "    def __theano_build__(self):\n",
    "        U, V, W = self.U, self.V, self.W\n",
    "        x = T.tensor3('x')\n",
    "        y = T.vector('y')\n",
    "        p = T.scalar('p')\n",
    "        \n",
    "        def dropout(X, p, srng_t_prev):\n",
    "            if T.lt(0.0, p):\n",
    "                X *= 1.0*srng_t_prev\n",
    "                X /= 1.0 - p\n",
    "            srng_t = 1.0*RandomStreams().binomial((self.hidden_dim, 1), p=1 - p)\n",
    "            return X, srng_t\n",
    "        \n",
    "        def forward_prop_step(x_t, s_t_prev, srng_t_prev, U, V, W, p):\n",
    "            s_t, srng_t = dropout(T.nnet.softplus(T.dot(U, x_t) + T.dot(W, s_t_prev)), p, srng_t_prev)  # T.tanh, T.nnet.relu\n",
    "            o_t = T.dot(V, s_t)\n",
    "#             for debugging ...\n",
    "#             print U.shape.eval()\n",
    "#             print V.shape.eval()\n",
    "#             print W.shape.eval()\n",
    "#             print (T.dot(U, x_t)).eval({x_t: [[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1]]})               \n",
    "#             print (T.dot(W, s_t_prev)).eval({s_t_prev: np.zeros((self.hidden_dim, 1))})\n",
    "#             print (T.dot(U, x_t) + T.dot(W, s_t_prev)).eval({x_t: [[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1]], \n",
    "#                             s_t_prev: np.zeros((self.hidden_dim, 1))})\n",
    "#             print (T.nnet.softplus(T.dot(U, x_t) + T.dot(W, s_t_prev))).eval({x_t: [[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1]], \n",
    "#                             s_t_prev: np.zeros((self.hidden_dim, 1))})\n",
    "#             print srng_t.eval()\n",
    "#             print s_t.eval({x_t: [[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1]], \n",
    "#                             s_t_prev: np.zeros((self.hidden_dim, 1)),\n",
    "#                             srng_t_prev: np.ones((self.hidden_dim, 1))})\n",
    "#             print o_t.eval({x_t: [[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1]], \n",
    "#                             s_t_prev: np.zeros((self.hidden_dim, 1)),\n",
    "#                            srng_t_prev: np.ones((self.hidden_dim, 1))})\n",
    "            return [o_t, s_t, srng_t]\n",
    "\n",
    "\n",
    "        [o,s,srng], updates = theano.scan(\n",
    "            fn=forward_prop_step,\n",
    "            sequences=[x],\n",
    "            outputs_info=[None, \n",
    "                          dict(initial=T.unbroadcast(T.zeros_like(T.reshape(V,[self.hidden_dim, 1], ndim=2)), 1)),  #dtype=theano.config.floatX \n",
    "                          dict(initial=T.unbroadcast(T.ones_like(T.reshape(V,[self.hidden_dim, 1], ndim=2)), 1))],  #dtype=theano.config.floatX \n",
    "            non_sequences=[U, V, W, 0.5],\n",
    "            truncate_gradient=self.bptt_truncate,\n",
    "            strict=True)\n",
    "        \n",
    "        [o_predict,s_predict,srng_predict], updates = theano.scan(\n",
    "            fn=forward_prop_step,\n",
    "            sequences=[x],\n",
    "            outputs_info=[None, \n",
    "                          dict(initial=T.unbroadcast(T.zeros_like(T.reshape(V,[self.hidden_dim, 1], ndim=2)), 1)),  #dtype=theano.config.floatX \n",
    "                          dict(initial=T.unbroadcast(T.ones_like(T.reshape(V,[self.hidden_dim, 1], ndim=2)), 1))],  #dtype=theano.config.floatX \n",
    "            non_sequences=[U, V, W, 0.0],  # predictions do not use dropout, so set p = 0.0\n",
    "            truncate_gradient=self.bptt_truncate,\n",
    "            strict=True)\n",
    "\n",
    "        \n",
    "        prediction = o_predict\n",
    "        o_error = T.sum(T.sqr(o - T.reshape(y, [self.t_dim,1,1], ndim=3)))\n",
    "        predict_error = T.sum(T.sqr(o_predict - T.reshape(y, [self.t_dim,1,1], ndim=3)))\n",
    "\n",
    "        # Gradients\n",
    "        dU = T.grad(o_error, U)\n",
    "        dV = T.grad(o_error, V)\n",
    "        dW = T.grad(o_error, W)\n",
    "        \n",
    "        # Assign functions\n",
    "        self.forward_propagation = theano.function([x], o)\n",
    "        self.predict = theano.function([x], prediction)\n",
    "        self.sse_error = theano.function([x, y], o_error)\n",
    "        self.predict_error = theano.function([x, y], predict_error)\n",
    "        self.bptt = theano.function([x, y], [dU, dV, dW])\n",
    "        \n",
    "        # SGD\n",
    "        learning_rate = T.scalar('learning_rate')\n",
    "        decay = T.scalar('decay')\n",
    "        \n",
    "        # rmsprop cache updates\n",
    "        mU = decay * self.mU + (1 - decay) * dU ** 2\n",
    "        mW = decay * self.mW + (1 - decay) * dW ** 2\n",
    "        mV = decay * self.mV + (1 - decay) * dV ** 2\n",
    "        \n",
    "        self.sgd_step = theano.function(\n",
    "            [x,y,learning_rate,theano.Param(decay, default=0.9)], \n",
    "            [], \n",
    "            updates=[(U, U - learning_rate * dU / T.sqrt(mU + 1e-6)),\n",
    "                     (W, W - learning_rate * dW / T.sqrt(mW + 1e-6)),\n",
    "                     (V, V - learning_rate * dV / T.sqrt(mV + 1e-6)),\n",
    "                     (self.mU, mU),\n",
    "                     (self.mW, mW),\n",
    "                     (self.mV, mV),\n",
    "                    ])\n",
    "\n",
    "\n",
    "    def calculate_total_loss(self, X, Y, predict=False):\n",
    "        if predict:\n",
    "            return np.sum([self.predict_error(x,y) for x,y in zip(X,Y)])            \n",
    "        else:\n",
    "            return np.sum([self.sse_error(x,y) for x,y in zip(X,Y)])\n",
    "    \n",
    "    def calculate_loss(self, X, Y, predict=False):\n",
    "        # Divide calculate_loss by the number of examples\n",
    "        num_examples = np.sum([len(y) for y in Y])\n",
    "        if predict:\n",
    "            return self.calculate_total_loss(X,Y,True)/float(num_examples)   \n",
    "        else:\n",
    "            return self.calculate_total_loss(X,Y,False)/float(num_examples)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Theano RNN for Yield Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time: 2016-04-27 13:55:58\n",
      "SGD Step time: 1.466036 milliseconds\n",
      "Epochs: 10, TrainingSamples: 958, ModelTime: 20160427135615\n",
      "1) ExamplesSeen=862, TrainLoss=0.3416, ValLoss=0.4367, ValRMSE=0.6609, TrainTime=0.01min, EndTime=01:56:16 saved\n",
      "2) ExamplesSeen=1724, TrainLoss=0.3382, ValLoss=0.3971, ValRMSE=0.6302, TrainTime=0.01min, EndTime=01:56:17 saved\n",
      "3) ExamplesSeen=2586, TrainLoss=0.4439, ValLoss=0.4256, ValRMSE=0.6524, TrainTime=0.01min, EndTime=01:56:18 \n",
      "4) ExamplesSeen=3448, TrainLoss=0.3898, ValLoss=0.4904, ValRMSE=0.7003, TrainTime=0.01min, EndTime=01:56:19 \n",
      "5) ExamplesSeen=4310, TrainLoss=0.4231, ValLoss=0.4389, ValRMSE=0.6625, TrainTime=0.01min, EndTime=01:56:20 \n",
      "6) ExamplesSeen=5172, TrainLoss=0.3626, ValLoss=0.4052, ValRMSE=0.6365, TrainTime=0.01min, EndTime=01:56:21 \n",
      "7) ExamplesSeen=6034, TrainLoss=0.4346, ValLoss=0.4156, ValRMSE=0.6447, TrainTime=0.01min, EndTime=01:56:23 \n",
      "8) ExamplesSeen=6896, TrainLoss=0.4199, ValLoss=0.4557, ValRMSE=0.6750, TrainTime=0.01min, EndTime=01:56:24 \n",
      "9) ExamplesSeen=7758, TrainLoss=0.3778, ValLoss=0.3925, ValRMSE=0.6265, TrainTime=0.01min, EndTime=01:56:25 saved\n",
      "10) ExamplesSeen=8620, TrainLoss=0.3488, ValLoss=0.4004, ValRMSE=0.6327, TrainTime=0.01min, EndTime=01:56:26 \n",
      "Finished.  Total train time = 0.00 hours\n",
      "End Time: 2016-04-27 13:56:26\n"
     ]
    }
   ],
   "source": [
    "#! /usr/bin/env python\n",
    "# Make sure the latest code updates are loaded\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from utils import *\n",
    "from rnn_theano_yp import RNNTheanoYP\n",
    "\n",
    "_X_DIM = int(os.environ.get('X_DIM', '23')) # number of features in the x_t vector\n",
    "_TIME_DIM = int(os.environ.get('TIME_DIM', '13')) # number of time periods\n",
    "_HIDDEN_DIM = int(os.environ.get('HIDDEN_DIM', '100'))\n",
    "_LEARNING_RATE = float(os.environ.get('LEARNING_RATE', '0.001'))\n",
    "_DECAY_RATE = float(os.environ.get('DECAY_RATE', '0.90'))\n",
    "_NEPOCH = int(os.environ.get('NEPOCH', '10'))\n",
    "_MODEL_FILE = os.environ.get('MODEL_FILE')\n",
    "\n",
    "def train_with_sgd(model, X_train, y_train, X_test, y_test, learning_rate=0.005, decay_rate=0.90, nepoch=1):\n",
    "    # Set start of training time\n",
    "    start_time = time.time()\n",
    "    # Set model time\n",
    "    modeltime = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    print 'Epochs: %d, TrainingSamples: %s, ModelTime: %s' %(nepoch, y_train.shape[0], modeltime)        \n",
    "    # Set epoch variables\n",
    "    min_loss = 100000\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    num_examples_seen = 0\n",
    "    # divide the data\n",
    "    train_data, train_labels, val_data, val_labels = shuffle_data(0.9, X_train, y_train)  # Use this for full sample training  \n",
    "#     train_data, train_labels, val_data, val_labels = shuffle_data2(X_train, y_train, 200000, 20000)  # Use this for testing and debugging  \n",
    "    for epoch in range(nepoch):\n",
    "#         # Adjust the learning rate if loss increases\n",
    "#         if (len(train_losses) > 1 and train_losses[-1][1] >= train_losses[-2][1]):\n",
    "#             learning_rate = learning_rate * 0.80  \n",
    "#             print \"Setting learning rate to %f\" % learning_rate\n",
    "        # Shuffle the training data\n",
    "        shuffle = np.random.permutation(np.arange(train_data.shape[0]))\n",
    "        train_data, train_labels = train_data[shuffle], train_labels[shuffle]\n",
    "        # Train the model\n",
    "        epoch_start = time.time()\n",
    "        # for each training example ...\n",
    "        for i in range(len(train_data)):\n",
    "            # One SGD step\n",
    "            model.sgd_step(train_data[i], train_labels[i], learning_rate, decay_rate)\n",
    "            num_examples_seen += 1\n",
    "        epoch_time = time.time() - epoch_start        \n",
    "        # Calculate training loss\n",
    "        train_loss = model.calculate_loss(train_data, train_labels, predict=False)\n",
    "        train_losses.append((num_examples_seen, train_loss))\n",
    "        # Calculate validation loss, R-squared, and RMSE\n",
    "        val_loss = model.calculate_loss(val_data, val_labels, predict=True)\n",
    "        val_losses.append((num_examples_seen, val_loss))\n",
    "        val_predictions = []\n",
    "        for i in range(len(val_data)):\n",
    "            val_predictions.append(model.predict(val_data[i]))\n",
    "        val_predictions = np.reshape(val_predictions, (len(val_predictions), len(val_predictions[0])))\n",
    "        val_R_squared = 1 - np.sum(np.square(val_predictions - val_labels))/np.sum(np.square(val_labels - np.mean(val_labels)))\n",
    "        val_rmse = np.sqrt(np.mean(np.square(val_predictions - val_labels)))\n",
    "        # If validation loss is a new minimum, save predictions and model   \n",
    "        is_saved = ''\n",
    "        if val_loss < min_loss:\n",
    "            min_loss = val_loss\n",
    "            # Make and save predictions\n",
    "            X_test_predictions = []\n",
    "            for i in range(len(X_test)):\n",
    "                label = float(y_test[i][0])\n",
    "                X_test_predictions.append(np.append(model.predict(X_test[i]).reshape(len(X_test[i])), label))\n",
    "            predictions_and_labels = np.asarray(X_test_predictions)\n",
    "            filename = \"./predictions/RNNs/pred-%s.txt\" %(modeltime)\n",
    "            np.savetxt(filename, predictions_and_labels, fmt='%.18f', delimiter=',',)\n",
    "            # Save model parameters\n",
    "            filename = \"./models/RNNs/RNN-%s.npz\" % (modeltime)\n",
    "            save_model_parameters_theano_yp(filename, model)\n",
    "            is_saved = 'saved'\n",
    "        # Print epoch stats\n",
    "        print '%d) ExamplesSeen=%d, TrainLoss=%.4f, ValLoss=%.4f, ValRMSE=%.4f, TrainTime=%.2fmin, EndTime=%s %s' %(epoch+1, num_examples_seen, train_loss, val_loss, val_rmse, epoch_time/60, time.strftime(\"%I:%M:%S\"), is_saved)\n",
    "    print 'Finished.  Total train time = %.2f hours' %((time.time() - start_time)/3600)   \n",
    "    \n",
    "    return train_losses, val_losses\n",
    "        \n",
    "# Start Time\n",
    "print 'Start Time: %s' %(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "# Build the model\n",
    "model = RNNTheanoYP(x_dim=_X_DIM, t_dim=_TIME_DIM, hidden_dim=_HIDDEN_DIM)\n",
    "\n",
    "# Measure and Print SGD step time\n",
    "t1 = time.time()\n",
    "model.sgd_step(X_train[10], Y_train[10], _LEARNING_RATE, _DECAY_RATE)\n",
    "t2 = time.time()\n",
    "print \"SGD Step time: %f milliseconds\" % ((t2 - t1) * 1000.)\n",
    "\n",
    "# if _MODEL_FILE != None:\n",
    "#     load_model_parameters_theano_yp(_MODEL_FILE, model)\n",
    "\n",
    "# Train the model\n",
    "train_losses, val_losses = train_with_sgd(model, X_train, log_Y_train, X_test, Y_test, learning_rate=_LEARNING_RATE, decay_rate = _DECAY_RATE, nepoch=_NEPOCH)\n",
    "\n",
    "# End Time\n",
    "print 'End Time: %s' %(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model:  The Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-Squared: -0.0045, MAE: 0.9071, RMSE: 1.1617, MAPE: 71.53%\n"
     ]
    }
   ],
   "source": [
    "Y_train_1d = Y_train[:,0]\n",
    "y_hat = len(Y_test)*[np.mean(Y_train_1d)]\n",
    "y_test = Y_test[:,0]\n",
    "\n",
    "y_hat = np.array(y_hat).astype(np.float)\n",
    "y_test = np.array(y_test).astype(np.float)\n",
    "y_bar = np.mean(y_test)\n",
    "\n",
    "R_squared = 1 - np.sum(np.square(y_hat - y_test))/np.sum(np.square(y_test - y_bar))\n",
    "mae = np.mean(np.abs(y_hat - y_test))\n",
    "rmse = np.sqrt(np.mean(np.square(y_hat - y_test)))\n",
    "mape = np.mean(abs(np.divide((y_hat - y_test), y_test)))\n",
    "\n",
    "print 'R-Squared: %.4f, MAE: %.4f, RMSE: %.4f, MAPE: %.2f%s' %(R_squared, mae, rmse, mape*100, '%')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Performance Stats on Holdout Sample\n",
    "These are the predictions on a holdout sample (X_test above) using the model with the lowest validation loss.    \n",
    "- t-0 = prediction at harvest time   \n",
    "- t-6 = prediction three months out from harvest   \n",
    "- t-12 = prediction six months out from harvest   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-Transformed Yield Results:\n",
      "t-12) R-Squared: -0.0801, MAE: 0.4295, RMSE: 0.6079, MAPE: 98.81%\n",
      "t-11) R-Squared: -0.0779, MAE: 0.4693, RMSE: 0.6073, MAPE: 85.27%\n",
      "t-10) R-Squared: -0.0157, MAE: 0.4427, RMSE: 0.5895, MAPE: 85.29%\n",
      "t-9) R-Squared: 0.0329, MAE: 0.4245, RMSE: 0.5752, MAPE: 84.71%\n",
      "t-8) R-Squared: 0.0701, MAE: 0.4122, RMSE: 0.5640, MAPE: 85.24%\n",
      "t-7) R-Squared: 0.0769, MAE: 0.4054, RMSE: 0.5620, MAPE: 87.77%\n",
      "t-6) R-Squared: 0.1175, MAE: 0.3942, RMSE: 0.5495, MAPE: 88.15%\n",
      "t-5) R-Squared: 0.1699, MAE: 0.3772, RMSE: 0.5329, MAPE: 83.15%\n",
      "t-4) R-Squared: 0.1704, MAE: 0.3733, RMSE: 0.5328, MAPE: 79.52%\n",
      "t-3) R-Squared: 0.1665, MAE: 0.3609, RMSE: 0.5340, MAPE: 81.92%\n",
      "t-2) R-Squared: 0.1626, MAE: 0.3645, RMSE: 0.5353, MAPE: 81.30%\n",
      "t-1) R-Squared: 0.1240, MAE: 0.3734, RMSE: 0.5475, MAPE: 82.81%\n",
      "t-0) R-Squared: 0.1188, MAE: 0.3743, RMSE: 0.5491, MAPE: 84.08%\n",
      "\n",
      "Values Transformed Back to Original Units Results:\n",
      "t-12) R-Squared: -0.2632, MAE: 1.0485, RMSE: 1.3028, MAPE: 68.35%\n",
      "t-11) R-Squared: -0.3716, MAE: 1.0887, RMSE: 1.3575, MAPE: 57.03%\n",
      "t-10) R-Squared: -0.2884, MAE: 1.0347, RMSE: 1.3158, MAPE: 55.83%\n",
      "t-9) R-Squared: -0.1823, MAE: 0.9923, RMSE: 1.2604, MAPE: 55.35%\n",
      "t-8) R-Squared: -0.1253, MAE: 0.9710, RMSE: 1.2297, MAPE: 55.40%\n",
      "t-7) R-Squared: -0.1189, MAE: 0.9580, RMSE: 1.2261, MAPE: 55.55%\n",
      "t-6) R-Squared: -0.0371, MAE: 0.9287, RMSE: 1.1805, MAPE: 54.17%\n",
      "t-5) R-Squared: 0.0522, MAE: 0.8881, RMSE: 1.1285, MAPE: 52.53%\n",
      "t-4) R-Squared: 0.0559, MAE: 0.8802, RMSE: 1.1263, MAPE: 52.67%\n",
      "t-3) R-Squared: 0.1016, MAE: 0.8441, RMSE: 1.0987, MAPE: 53.46%\n",
      "t-2) R-Squared: 0.1119, MAE: 0.8522, RMSE: 1.0924, MAPE: 54.20%\n",
      "t-1) R-Squared: 0.0497, MAE: 0.8781, RMSE: 1.1300, MAPE: 56.41%\n",
      "t-0) R-Squared: 0.0657, MAE: 0.8781, RMSE: 1.1204, MAPE: 57.29%\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "y_hat = []\n",
    "Y_test = []\n",
    "with open('./predictions/RNNs/pred-20160427135615.txt', 'r') as csvfile:\n",
    "    datareader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in datareader:\n",
    "        label = row.pop()\n",
    "        y_hat.append(row)\n",
    "        Y_test.append(label)\n",
    "\n",
    "# Results keeping predictions as logs of yield\n",
    "y_hat = np.array(y_hat).astype(np.float)\n",
    "Y_test = np.array(Y_test).astype(np.float)\n",
    "y_test = np.log(Y_test)\n",
    "y_bar = np.mean(y_test)\n",
    "\n",
    "print 'Log-Transformed Yield Results:'\n",
    "for j in range(len(y_hat[0])):\n",
    "    y_hat_last = y_hat[:, j]\n",
    "    R_squared = 1 - np.sum(np.square(y_hat_last - y_test))/np.sum(np.square(y_test - y_bar))\n",
    "    mae = np.mean(np.abs(y_hat_last - y_test))\n",
    "    rmse = np.sqrt(np.mean(np.square(y_hat_last - y_test)))\n",
    "    count = 0\n",
    "    sums = 0\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test[i] != 0: \n",
    "            error = np.divide(abs(y_hat_last[i] - y_test[i]), abs(y_test[i]))\n",
    "            count += 1\n",
    "            sums += error\n",
    "    mean_abs_pct_error = sums/count\n",
    "    print 't-%d) R-Squared: %.4f, MAE: %.4f, RMSE: %.4f, MAPE: %.2f%s' %(len(y_hat[0])-1-j, R_squared, mae, rmse, mean_abs_pct_error*100, '%')\n",
    "    \n",
    "    \n",
    "# Results transforming predictions and yields back to original units\n",
    "y_hat = np.array(y_hat).astype(np.float)\n",
    "y_test = np.array(Y_test).astype(np.float)\n",
    "y_bar = np.mean(y_test)\n",
    "\n",
    "y_hat_exp = np.exp(y_hat)\n",
    "\n",
    "print '\\nValues Transformed Back to Original Units Results:'\n",
    "for j in range(len(y_hat_exp[0])):\n",
    "    y_hat_last = y_hat_exp[:, j]\n",
    "    R_squared = 1 - np.sum(np.square(y_hat_last - y_test))/np.sum(np.square(y_test - y_bar))\n",
    "    mae = np.mean(np.abs(y_hat_last - y_test))\n",
    "    rmse = np.sqrt(np.mean(np.square(y_hat_last - y_test)))\n",
    "    count = 0\n",
    "    sums = 0\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test[i] != 0: \n",
    "            error = np.divide(abs(y_hat_last[i] - y_test[i]), abs(y_test[i]))\n",
    "            count += 1\n",
    "            sums += error\n",
    "    mean_abs_pct_error = sums/count\n",
    "    print 't-%d) R-Squared: %.4f, MAE: %.4f, RMSE: %.4f, MAPE: %.2f%s' %(len(y_hat_exp[0])-1-j, R_squared, mae, rmse, mean_abs_pct_error*100, '%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsQAAAF/CAYAAABOsN+oAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQAwAAABf+7+zqDHTbCm8+wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1132b7850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_losses(train_losses, val_losses, epochs, y_min, y_max):\n",
    "    x1 = range(1, epochs+1, 1)\n",
    "    y1 = train_losses\n",
    "    x2 = range(1, epochs+1, 1)\n",
    "    y2 = val_losses\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.axis((1, epochs, y_min, y_max))\n",
    "    plt.plot(x1, y1, \"-m\", linewidth=2.5, label='Train Loss') \n",
    "    plt.plot(x2, y2, \"-b\", linewidth=1.0, label='Validation Loss')\n",
    "    plt.legend(['Train Loss', 'Validation Loss'])\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.show()\n",
    "\n",
    "plot_losses(train_losses, val_losses, epochs=10, y_min=0.0, y_max=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for Reloading the Saved Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model parameters from ./models/RNNs/RNN-20160427131504.npz. hidden_dim=100 x_dim=23, t_dim=13\n"
     ]
    }
   ],
   "source": [
    "reloaded_model = load_model_parameters_theano_yp(\"./models/RNNs/RNN-20160427131504.npz\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
